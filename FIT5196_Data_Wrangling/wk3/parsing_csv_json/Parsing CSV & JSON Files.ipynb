{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parsing CSV & JSON Files\n",
    "\n",
    "Due to advances in technologies for data storage, data from various sources is always stored in different formats\n",
    "and file types. \n",
    "Some data formats store data in a way that can be easily handled by a machine, such as CSV, JSON, and XML.\n",
    "Those formats are usually referred to as machine-readable formats.\n",
    "In contrast, some other data formats or file types store data in a way meant to be read by a human \n",
    "using front-end desktop tools.\n",
    "Those formats or file types are often referred to as hard-to-parse formats.\n",
    "We will use a series of examples to demonstrate how to extract data stored in \n",
    "both machine-readable and hard-to-parse formats,\n",
    "and then store the extracted data in formats that can be easily adopted by the downstream data wranngling tasks.\n",
    "This chapter will cover how to read the common machine-readable formats:\n",
    "* **CSV**: Comma Separated Values\n",
    "* **JSON**: JavaScript Object Notation\n",
    "\n",
    "In most cases, the two formats togeather with XML are the best available resource while you are scraping data from\n",
    "the web or requesting data directly from an organization or agency. \n",
    "They are more easily used and ingested by programming languages, like Python.\n",
    "Our suggestion is that you should try your best to get data in these formats, before you start looking\n",
    "into other formats that might be hard to parse, like PDFs.\n",
    "\n",
    "There are many ways of reading and storing data in those formats, \n",
    "which depends on the programming language you use.\n",
    "Here we are going to focus on Python.\n",
    "Searching the Internet, you will find there are a lot of online tutorials on handling data stored in different\n",
    "data formats with Python.\n",
    "We suggest the following:\n",
    "* \"*Data Loading, Storage, and File Formats*\", Chapter 6 of \"**Python for Data Analysis**\": This chapter covers reading files in a variety of formats, loading data from databases and interacting with Internet via APIs. Please read pages 155-166, and download and run the Python scripts from [the author's github site](https://github.com/pydata/pydata-book). ðŸ“–\n",
    "\n",
    "The dataset used in this chapter was downloaded from\n",
    "[data.gov.au](https://data.melbourne.vic.gov.au/Transport-Movement/Melbourne-bike-share/tdvh-n9dv). \n",
    "It is available in the following formats: CSV, JSON, XML, RDF, etc.\n",
    "The first two formats are used, i.e., the following two files\n",
    "* Melbourne_bike_share.csv\n",
    "* Melbourne_bike_share.json\n",
    "\n",
    "In the following sections, you will learn how to scrape data from the two \n",
    "example files, and store the extracted data into Pandas DataFrame. \n",
    "\n",
    "### Example scenario\n",
    "Assume that you are going to analyze and predict bicycle hubway station status to answer the following questions:\n",
    "* What do usage patterns look like with respect to specific stations and how that translates to imbalances in the system?\n",
    "* Can we integrate these explanatory variables and these usage patterns into a predictive algorithm that would predict empty and full stations in the near future?\n",
    "* What form should that algorithm take?\n",
    "* How do environmental variables affect the future state of Hubway stations?\n",
    "\n",
    "See <a href=\"http://cs109hubway.github.io/classp/\"><font color=\"red\">Predicting Hubway Stations status in Boston</font></a> for more discussion.\n",
    "\n",
    "The first step we have to do is to acquire the hub station data and as well as weather data. Here, for demonstration purpose, we use the Melbourne bike share data published by the government. The files have been downloaded and come along with this notebook.\n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing CSV file\n",
    "A CSV is a Comma Separated Values file, which allows data to be saved in a tabular format.\n",
    "Each row of the file is a data record; each column is a field (or an attribute).\n",
    "Each data record consists of one or more fields, separated by commas.\n",
    "As one of the most popular file formats,\n",
    "it is supported by any spreadsheet programs, such as \n",
    "Microsoft Excel, Open Office Calc, and Google Spreadsheets,\n",
    "Because of its simplicity,\n",
    "it differs from other spreadsheet file types, such as Excel, in that one can only store a single sheet in a file. \n",
    "It cannot be used to store cell, columns or row styling, figures and formulas.\n",
    "To make our CSV file, i.e., Melbourne_bike_share.csv, easier to view here, \n",
    "a sample of the data with trimmed down records is shown below.\n",
    "You should see something similar to this when you open the excel file in your text editor,\n",
    "![csv1.png](./csv1.png)\n",
    "\n",
    "Note that tabs can also be used to separate values of different fields.\n",
    "This type of files is usually called TSV, Tab Separated Values. \n",
    "Sometimes TSVs get classified as CSVs.\n",
    "The only difference between CSVs and TSVs is the delimiter.\n",
    "Essentially, the two types of files will act the same in Python and most of the other\n",
    "programming languages. \n",
    "It is worth mentioning that they often take the form of a text file containing information \n",
    "separated by commas.\n",
    "This section will show you how to use Pandas \n",
    "[read_csv()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) function to\n",
    "load our CSV file, and how to tidy the loaded data a bit.\n",
    "Before we start importing our CSV file, it might be good for you to read [Pandas tutorial\n",
    "on reading CSV files](http://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table) ðŸ“–."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach to inspect your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./Melbourne_bike_share.csv\", 'r') as f:\n",
    "    for line in f.readlines()[:10]:\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./Melbourne_bike_share.csv\", 'r') as f:\n",
    "    for line in f.readlines()[-10:]:\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing CSV data\n",
    "Importing CSV files with Pandas <a href='http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html'><font color = \"blue\">read_csv()</font></a> function and converting the data into a form Python can understand \n",
    "is simple. \n",
    "It only takes a couple of lines of code.\n",
    "The imported data will be stored in Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csvdf = pd.read_csv(\"./Melbourne_bike_share.csv\")\n",
    "type(csvdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use the <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html\"><font color='blue'>read_table()</font></a> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf_1 = pd.read_table(\"./Melbourne_bike_share.csv\", sep=\",\")\n",
    "type(csvdf_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data should be loaded into Python. \n",
    "Let's have a look at the first 5 records in the dataset.\n",
    "There are a coupe of ways to retrieve these records.\n",
    "For example, you can use \n",
    "* <font color='blue'>csvdf.head(n = 5)</font>: It will return first `n` rows in a DataFrame, n = 5 by default.\n",
    "* <font color='blue'>csvdf[:5]</font>: It uses the slicing method to retrieve the first 5 rows\n",
    "\n",
    "Refer to \"[Indexing and Selecting Data](http://pandas.pydata.org/pandas-docs/stable/indexing.html)\"\n",
    "for how to slice, dice, and generally get and set subsets of pandas objects.\n",
    "Here, we use the `head` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf.head()\n",
    "#csvdf.loc[:4]\n",
    "#csvdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the row indices are integers automatically generated by Pandas.\n",
    "Suppose you want to set IDs as row indices and delete the ID column.\n",
    "Resetting the row indices can be easily done with the following DataFrame function\n",
    "```python\n",
    "    DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "```\n",
    "See its [API webpage](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html) \n",
    "for the detailed usage.\n",
    "The keys are going to be the IDs in the first column. \n",
    "By setting `inplace = True`, the corresponding change is done inplace and won't return a new DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(csvdf.ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf.set_index(csvdf.ID, inplace = True)\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the ID column that is now redundant, you use DataFrame `drop` function and set `inplace = True`\n",
    "```python\n",
    "    DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf.drop('ID', 1, inplace = True)\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the above method of setting row indices to IDs, you can specify which column to \n",
    "be used as row indices while reading the CSV file. See the API reference page for\n",
    "[pandas.read_csv](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).\n",
    "To do so, you can use the <font color='blue'>index_col</font> argument of <font color='blue'>read_csv()</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf = pd.read_csv(\"./Melbourne_bike_share.csv\", index_col = \"ID\")\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, with the <font color='blue'>read_table()</font> function, you can also set the value of <font color='blue'> index_col</font> to \"ID\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2. Manipulating the Data\n",
    "\n",
    "So far, you have learned a little bit about the Melbourne_bike_share data.\n",
    "Let's further process the data by splitting the coordinates into latitude and longitude.\n",
    "First figure out what type of data we're dealing with, i.e., the data type of the \"Coordinates\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(csvdf['Coordinates']) \n",
    "# type(csvdf.Coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type of this column is Pandas Series, i.e., \n",
    "a one-dimensional labeled array capable of holding any data type.\n",
    "Next, in order to split the coordinates, you should know the data type of those coordinates. Are they strings?\n",
    "Let's check them by printing the first element in the Series and its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (csvdf['Coordinates'].iloc[0])\n",
    "type(csvdf['Coordinates'].iloc[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Those coordinates are indeed strings. Thus, to extract both latitude and longitude, you \n",
    "can either use regular expressions introduced in the previous chapter or common string operations.\n",
    "\n",
    "To use regular expressions, the key is figuring out the patterns of characters. Then\n",
    "according to those patterns, you formulate your regular expressions.\n",
    "Looking at the first couple of coordinates in the Series object, i.e.:\n",
    "```\n",
    "    (-37.814022, 144.939521)\n",
    "    (-37.817523, 144.967814)\n",
    "    (-37.84782, 144.948196)\n",
    "```\n",
    "You will find that latitudes are always negative real values, and longitudes are positive real values.\n",
    "That is because Australia lies between latitudes 9Â° and 44Â°S, and longitudes 112Â° and 154Â°E.\n",
    "The regular expression is\n",
    "```\n",
    "    r\"-?\\d+\\.?\\d*\"\n",
    "```\n",
    "![](./regex1.jpg)\n",
    "It contains four parts\n",
    "* \"-?\": optionally matches a single '-'.\n",
    "* \"\\d+\": matches one or more digits.\n",
    "* \"\\\\.?\": optionally matches a single dot.\n",
    "* \"\\d*\": matches zero or more digits.\n",
    "\n",
    "The following code extracts all real values matching this regular expression.\n",
    "The <font color=\"blue\">re.findall()</font> returns all matched values in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "str1 = csvdf['Coordinates'].iloc[0] # csvdf.Coordinates\n",
    "re.findall(r\"-?\\d+\\.?\\d*\", str1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using common string operations might be simpler than using regular expressions. \n",
    "<font color=\"blue\">str.split()</font> is the function used here to extract both latitudes and longitudes.\n",
    "However, you should choose a proper delimiter to split a string.\n",
    "First, split the string by ',':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = csvdf['Coordinates'].iloc[1].split(', ') # assuming they're all '(x, y)'\n",
    "print ('lat = ', s[0], ' long = ', s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The printout shows that the latitude contains '(', and the longitude contains ')'.\n",
    "You should consider removing both the left and the right parentheses. \n",
    "Of course, the `split` function can be used again. \n",
    "Note that the goal here is to remove the leading and trailing parentheses.\n",
    "Python string class provides two functions to do the two operations,\n",
    "which are:\n",
    "* <font color=\"blue\">string.lstrip()</font>: returns a copy of the string with leading characters removed\n",
    "* <font color=\"blue\">string.rstrip()</font>: returns a copy of the string with trailing characters removed.\n",
    "\n",
    "Let's try the two functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (s[0].lstrip('('))\n",
    "print (s[1].rstrip(')'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latitude and longitude in the first coordinate have been successfully extracted.\n",
    "Next, we are going to apply the extracting process to every coordinate in the DataFrame.\n",
    "There are multiple ways of doing that. \n",
    "The most straightforward way is to write a FOR loop to iterate over all the coordinates,\n",
    "and apply the above scripts to each individual coordinate. \n",
    "Two Pandas Series can be then used to store latitudes and longitudes.\n",
    "However, we are going to show you how to use some advanced Python programming functionality.\n",
    "\n",
    "Pandas Series class implements an [`apply()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method that applies a given function\n",
    "to all values in a Series object, and returns a new one.\n",
    "Please note that this function can only works on single values. \n",
    "To apply <font color=\"blue\">str.split()</font> to every coordinate and\n",
    "get latitudes and longitudes, you can use the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf['lat'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[0])\n",
    "csvdf['lon'] = csvdf['Coordinates'].apply(lambda x: x.split(', ')[1])\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line extracts all the latitudes and store them in a column in our DataFrame.\n",
    "The second line extracts all the longitudes.\n",
    "You might wonder what \"lambda\" is in the code. \n",
    "It is a Python keyword used to construct small anonymous functions at runtime. (See [Section 4.7.5. Lambda Expressions](https://docs.python.org/2/tutorial/controlflow.html) ðŸ“– )\n",
    "You can use a similar approach to remove the heading and trailing parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf['lat'] = csvdf['lat'].apply(lambda x: x.lstrip('('))\n",
    "csvdf['lon'] = csvdf['lon'].apply(lambda x: x.rstrip(')'))\n",
    "csvdf.drop('Coordinates', 1, inplace = True)\n",
    "csvdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have split the \"Coordinates\" column into two columns, i.e., \"lat\" and 'lon' in the DataFrame,\n",
    "and dumped the \"Coordinates\" column.\n",
    "The last step is to infer better type for object columns. \n",
    "All the numerical values and dates are encoded as strings in the current DataFrame.\n",
    "We would like to convert those values to types that they are supposed to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvdf = csvdf.convert_objects(convert_numeric = True) \n",
    "csvdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, dates are still strings, which means the `convert_object` function cannot convert data strings to datatime\n",
    "object.\n",
    "Here you need to force them to be converted to datatime object with [`pd.to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvdf['UploadDate'] = pd.to_datetime(csvdf['UploadDate'])\n",
    "print (csvdf.dtypes)\n",
    "csvdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you have loaded the given CSV file into Python with Pandas. \n",
    "You have also tidied the data a bit by getting latitudes and longitudes out\n",
    "from the strings.\n",
    "\n",
    "Besides `read_csv`, there are other parsing functions in pandas for \n",
    "reading tabular data as a DataFrame object. They include\n",
    "* [`read_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html): Reads general delimited file into DataFrame. The default delimiter is '\\t'.\n",
    "* [`read_fwf`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_fwf.html): Reads a table of fixed-width formatted lines into DataFrame.\n",
    "* [`read_clipboard`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_clipboard.html): Reads text from clipboard and passes to read_table. See read_table for the full argument list.\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing JSON files\n",
    "\n",
    "JSON (JavaScript Object Notation) is one of the most commonly used formats \n",
    "for transferring data between web services and other applications via HTTP requests.\n",
    "Nowadays, many sites have JSON-enabled APIs and \n",
    "JSON is quickly becoming the encoding protocol of choice.\n",
    "As a light weighted data-interchange format inspired by JavaScript, \n",
    "it is clean, easy to read, and easy to parse.\n",
    "Here is a simple example adapted from [Wikipedia page on JSON](https://en.wikipedia.org/wiki/JSON)\n",
    "```\n",
    "[\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Smith\",\n",
    "  \"age\": 25,\n",
    "  \"address\": {\n",
    "    \"streetAddress\": \"21 2nd Street\",\n",
    "    \"city\": \"New York\",\n",
    "    \"state\": \"NY\",\n",
    "    \"postalCode\": \"10021\"\n",
    "   }\n",
    "}\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "From the above example, you will see that each data record looks like a [Python dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries). \n",
    "A JSON file usually contains a list of dictionaries, which is defined by '[' and ']'.\n",
    "In each of those dictionaries,\n",
    "there is a key-value pair for each row and the key and value are separated by a colon.\n",
    "Different key-value pairs are separated by commas.\n",
    "Note that a value can also be a dictionary, see \"address\" in the example.\n",
    "The basic types are object, array, value, string and number.\n",
    "If you would like to know more about JSON, please refer to \n",
    "* [Introducing to JSON](http://www.json.org/): the JSON org website gives a very good diagrammatic explanation \n",
    "of JSON ðŸ“–.\n",
    "* [Introduction to JSON](https://www.youtube.com/watch?v=WWa0cg_xMC8): a 15-minutes Youtube video on JSON, recommended for visual learners.\n",
    "\n",
    "(Of course, you can also go and find your own materials on JSON by searching the Internet.)\n",
    "\n",
    "In the rest of this section, we will start from an simple example, walking through steps of acquiring JSON Data from Google Maps Elevation API and normalizing those data into a flat table. Then, we revisit the dataset mentioned in the previous section (except that it is now in JSON format), parsing the data and store them in a Pandas DataFrame object.\n",
    "Before we start, it might be good for you to view one of the following tutorials on parsing JSON files:\n",
    "* [Working with JSON data](http://wwwlyndacom.ezproxy.lib.monash.edu.au/Python-tutorials/Working-JSON-data/122467/142575-4.html): A Lynda tutorial on parsing JSON data. You need a Monash account to access this website.\n",
    "[here](http://resources.lib.monash.edu.au/eresources/lynda-guide.pdf) is the lynda settup guide.\n",
    "* A [Youtube video](https://www.youtube.com/watch?v=9Xt2e9x4xwQ ) on extracting data from JSON files (**optional**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Acquiring JSON Data From The Internet\n",
    "This section will start with showing you how to acquire a small chunk of JSON data\n",
    "from Internet via HTTP requests and load it into Python with `json` library. \n",
    "The example we used is inspired by the question asked in [Stack Overflow](http://stackoverflow.com/questions/21104592/json-to-pandas-dataframe).\n",
    "In the example, the goal is to extract elevation data from a \n",
    "[Google Maps Elevation API](https://developers.google.com/maps/web-services/overview) along\n",
    "a path specified by latitude and longitude, and convert the JSON data\n",
    "into a Pandas DataFrame object, which could look similar to (but the actual values might vary!)\n",
    "\n",
    "||elevation|location.lat|location.lng|resolution|\n",
    "|------|------|------|------|------|\n",
    "|0|243.346268|42.974049|-81.205203|19.087904|\n",
    "|1|244.131866|42.974298|-81.195755|19.087904|\n",
    "\n",
    "\n",
    "The first step is to make a HTTP request to get the data from the Google Maps API.\n",
    "Here we are going to use [`urllib2`](https://docs.python.org/2/library/urllib2.html) library.\n",
    "It defines a set of functions and classes that help in opening URLs.\n",
    "\n",
    "In order to run the following code, please following the instruction on https://developers.google.com/maps/documentation/elevation/start\n",
    "to request a API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locations = \"42.974049,-81.205203|42.974298,-81.195755\"\n",
    "try:\n",
    "    from urllib2 import Request, urlopen # for python 2\n",
    "except ImportError:\n",
    "    from urllib.request import urlopen, Request # for python 3\n",
    "\n",
    "api_key = \"YOUR API-KEY\" #use your own API key her\n",
    "request = Request(\"https://maps.googleapis.com/maps/api/elevation/json?locations=\"+locations+\"&key=\"+api_key)\n",
    "\n",
    "response = urlopen(request)\n",
    "elevations = response.read()\n",
    "#elevations.splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we have:\n",
    "1. Imports Request class and the <font color=\"blue\">urlopen() </font> function from `urllibs` module.\n",
    "2. Defines a path with the coordinates of the start and end points\n",
    "3. Creates a URL Request object. Note that you can change the output format by replacing '/json' with '/xml'.\n",
    "4. Opens the URL, and returns a file-like object.\n",
    "5. Reads data returned from the HTTP request.\n",
    "\n",
    "The returned data is actually stored in a string. \n",
    "You can check it out using Python's built-in function `type`, \n",
    "```python\n",
    "    type(elevations)\n",
    "```\n",
    "What does the data look like?\n",
    "In stead of printing the data in one single string, one can use\n",
    "```python\n",
    "    elevations.splitlines()\n",
    "```\n",
    "to print the data as a list of lines in the string, breaking\n",
    "at line boundaries, i.e., '\\n'. \n",
    "The printout you get should look like\n",
    "```\n",
    "['{',\n",
    " '   \"results\" : [',\n",
    " '      {',\n",
    " '         \"elevation\" : 243.3462677001953,',\n",
    " '         \"location\" : {',\n",
    " '            \"lat\" : 42.974049,',\n",
    " '            \"lng\" : -81.205203',\n",
    " '         },',\n",
    " '         \"resolution\" : 19.08790397644043',\n",
    " '      },',\n",
    " '      {',\n",
    " '         \"elevation\" : 244.1318664550781,',\n",
    " '         \"location\" : {',\n",
    " '            \"lat\" : 42.974298,',\n",
    " '            \"lng\" : -81.19575500000001',\n",
    " '         },',\n",
    " '         \"resolution\" : 19.08790397644043',\n",
    " '      }',\n",
    " '   ],',\n",
    " '   \"status\" : \"OK\"',\n",
    " '}']\n",
    "```\n",
    "It is easy to dump the data into a JSON file, which just takes three lines of code:\n",
    "```python\n",
    "    import json\n",
    "    with open(\"elevations.json\", \"w\") as outfile:\n",
    "         json.dump(elevations, outfile)\n",
    "```\n",
    "\n",
    "To read the acquired JSON data, you can use the `json` module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(elevations)\n",
    "print (type(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It loads the data into a Python dictionary.\n",
    "The data we want is stored in the first entry.\n",
    "The value of this entry is a list of two dictionaries, each of which corresponds to a record.\n",
    "see [JSON encoder and decoder](https://docs.python.org/2/library/json.html) for more on reading\n",
    "JSON files.\n",
    "\n",
    "As mentioned earlier in this section, \n",
    "we will convert the JSON data into Pandas DataFrame.\n",
    "Therefore, Pandas functions on reading JSON are to be used.\n",
    "If you would like to know about those functions, you can read Pandas tutorial on [Reading JSON](http://pandas.pydata.org/pandas-docs/stable/io.html#io-json-reader) (**optional**).\n",
    "Let's first try the <font color=\"blue\">read_json()</font> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(elevations)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the DataFrame returned by `read_json` is not the one we want.\n",
    "You might wonder why the `read_json` function did not return the DataFrame we want.\n",
    "There is a straight forward answer.\n",
    "Let's try to build a DataFrame from `data` returned by \n",
    "```\n",
    "    data = json.loads(elevations)\n",
    "```\n",
    "What do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have got a DataFrame that is exactly the same as the one returned by `read_json`.\n",
    "This is due to Pandas' way of constructing a DataFrame from a dictionary. \n",
    "See [Intro to Data Structures](http://pandas.pydata.org/pandas-docs/stable/dsintro.html)\n",
    "for constructing a DataFrame from a dictionary\n",
    "and \"Object Creation\" in [10 Mintues to Pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html) ðŸ“–.\n",
    "It is not hard to figure out that dictionary keys \n",
    "are used as column \n",
    "labels, and values of whatever data types are put as column values.\n",
    "\n",
    "What we want is to flatten out JSON object into a flat table.\n",
    "Fortunately, Pandas provides a JSON normalization function [(<font color=\"blue\">json_normalize()</font>)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html)\n",
    "that takes a dict or list of dicts and normalize semi-structured data into a flat table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "json_normalize(data['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the <font color=\"blue\">json_normalize()</font> function returns the DataFrame we want.\n",
    "However flattening objects with embedded arrays/lists is not as trivial.\n",
    "See [Flattening JSON objects in Python](https://gist.github.com/amirziai/2808d06f59a38138fa2d)\n",
    "for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Parsing the \"Melbourne_bike_share.json\"  File\n",
    "Now that you have learned how to use `json` module and Pandas together to parse a simple JSON file.\n",
    "In this section we will walk you through the process of extracting bike hub station statistical data from \"Melbourne_bike_share.json\". Then produce the same DataFrame as the one in Section 1.\n",
    "\n",
    "Remember that the first step is always to glance through the JSON file with your favorite editor.\n",
    "Below is the first 20 lines from our JSON file.\n",
    "\n",
    "<img src = \"./json20.png\" width = \"700\", hight = \"800\">\n",
    "\n",
    "This JSON file is much more complex that the one used in the previous section\n",
    "It might take a bit of time to figure out that this file is a dictionary of \n",
    "two large dictionaries, one with key \"meta\", and another with \"data\".\n",
    "The \"meta\" dictionary contains all the meta information, including column names.\n",
    "The \"data\" dictionary actually contains the data we want.\n",
    "In the following subsection, we will show you how to extract records from the \"data\"\n",
    "dictionary, while leaving the task of extracting column labels from the \"meta\" dictionary as an exercise.\n",
    "Similarly, our JSON data can be read into Python as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./Melbourne_bike_share.json\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "print (type(json_data))\n",
    "json_data['meta']['view']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded JSON data has been saved in a Python dictionary with two entries, one for \"data\" and another for \"meta\".\n",
    "Using `json_normalize`, you can flatten the \"data\" dictionary into a table and save it in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = json_normalize(json_data,'data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have a lot of extra columns.\n",
    "The data we want starts at column 8.\n",
    "Therefore, dump all the irrelevant preceding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df.drop(xrange(8), axis=1, inplace=True)\n",
    "except:\n",
    "    df.drop(range(8), axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Renaming all the columns with the field names given by the CSV file. \n",
    "You can programmatically extract field names from the \"meta\" dictionary.\n",
    "We will leave it for you to do as an exercise.\n",
    "Similar to parsing CSV file, IDs are unique and can be set to row indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['id','featurename','terminalname','nbbikes','nbemptydoc','uploaddate','coordinates']\n",
    "df.set_index(df.id, inplace= True)\n",
    "df.drop('id', 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What's in the last two columns?\n",
    "\"uploaddate\" is supposed to have a standard datetime format in the column,\n",
    "and coordinates should be pairs of latitude and longitude.\n",
    "Both of them should be real numbers.\n",
    "At the moment, a datetime is encoded as a 64-digit integer (i.e., datetimes in milliseconds since epoch),\n",
    "and a coordinate is a Python list as\n",
    "```python\n",
    " [u'{\"address\":\"\",\"city\":\"\",\"state\":\"\",\"zip\":\"\"}',\n",
    " u'-37.814022',\n",
    " u'144.939521',\n",
    " None,\n",
    " False]\n",
    "```\n",
    "Let's first convert those integers into standard datetime.\n",
    "The following Python code converts \n",
    "one of these integers into a standard datetime using Python\n",
    "[`datatime`](https://docs.python.org/2/library/datetime.html) module:\n",
    "```python\n",
    "    import datatime\n",
    "    date = datetime.datetime.fromtimestamp(df.iloc[0,4])\n",
    "    print data\n",
    "```\n",
    "The output is \n",
    "```\n",
    "    2016-01-28 23:45:05\n",
    "```\n",
    "Similar to the way of splitting coordinates in Section 2.1, \n",
    "one can use `pandas.Series.apply` to invoke  `datetime.datetime.fromtimestamp`\n",
    "on each individual integer in the column. \n",
    "Please try this method by yourself.\n",
    "\n",
    "Instead, we will show you a pandas specific way of converting \n",
    "timestamp values in milliseconds into standard datetime.\n",
    "Here we use Pandas [`to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html)\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['uploaddate'] = pd.to_datetime(df['uploaddate'], unit='s')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the unit argument must be explicitly specified. It can take values on (D,s,ms,us,ns).\n",
    "Without specifying its value, `1453985105`, for example, will be converted to some strange date as\n",
    "```\n",
    "    Timestamp('1970-01-01 00:00:01.453985105')\n",
    "```\n",
    "You can compare the converted dates with those in the DataFrame constructed from our CSV file.\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (csvdf.iloc[0,4]) # the csv date\n",
    "print (df.iloc[0,4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is due to that two files were downloaded one after another.\n",
    "However, the time format is the same.\n",
    "\n",
    "The last step is to extract latitudes and longitudes into two columns.\n",
    "Each coordinate in the last column of the DataFrame is a Python list.\n",
    "The second and the third entries are latitude and longitude respectively.\n",
    "It is very easy to get the two entries into a list.\n",
    "We will apply the following anonymous function to all the coordinates one after another\n",
    "```python\n",
    "    lambda col: col[i]\n",
    "```\n",
    "where i = 1 or 2. While i = 1, it returns latitudes; i = 2, it returns longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['lat'] = df['coordinates'].apply(lambda col: col[1]) # arrrrgh\n",
    "df['lon'] = df['coordinates'].apply(lambda col: col[2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, dump the \"coordinates\" columns and change data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('coordinates', 1, inplace = True)\n",
    "df = df.convert_objects(convert_numeric=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "\n",
    "Files in either CSV or JSON format are the easiest ones to preview, understand and parse. \n",
    "In this chapterï¼Œyou have learned about how to pull data out from files stored in those two formats\n",
    "using Pandas. You should now be familiar with these two formats.\n",
    "\n",
    "## Exercises\n",
    "1. To further parse the Excel file, try the following \n",
    "    1. Split the \"Featurename\" into bike hub station's street name, and suburb name, then store them in three columns.\n",
    "    2. Extract date and time from the \"UploadDate\" columns, store them in two different columns.\n",
    "2.  Section 3.2 has shown you how to extract data from the given JSON file. However, it did not show\n",
    "how to programmatically extract column labels from the meta data. The task here is to extract all\n",
    "the column labels from the metadata using either <font color='blue'>json_normalize()</font> function or the way you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "2. Parsing CSV & JSON Files.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
