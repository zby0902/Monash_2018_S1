{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing XML Patents \n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This tutorial forcus on preprocessing a set of patents documents stored in XML format and generating the sparse representations for those patents. The final output file should be exactly the same as the one stored in \"patents.txt\".\n",
    "\n",
    "In order to finish this task, you should \n",
    "1. Exatract the abstract and claims for each patent from its xml file. Use Beautiful soup \n",
    "2. Tokenise the patents\n",
    "3. Generate 100 bigram collocations \n",
    "4. Re-tokenize the patents with those bigram collocations\n",
    "5. Generate the TF-IDF vectors for those re-tokenized patents\n",
    "6. save the vectors in the form shown in \"patents.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries \n",
    "\n",
    "Here we will focus on using the existing packages as possible as we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exatract Patent's abstract and Claims\n",
    "\n",
    "The first task is to parse each patent stored in the \"xml_files\" folder. The information to be extracted includes\n",
    "1. patent document number (doc-number) stored in \"publication-reference\"\n",
    "2. patent's abstract\n",
    "3. patent's claims \n",
    "\n",
    "Hint: you can use a dictionary to save patents, where the key is the doc-number, the value is a long string contains both abstracts and all claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml_file_path = \"./xml_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(t):\n",
    "\n",
    "    xmlSoup = bsoup(t,\"lxml-xml\")\n",
    "    \n",
    "    pid = xmlSoup.find(\"publication-reference\").find('doc-number').string \n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    #Extract text in \"abstract\"    \n",
    "    abt = xmlSoup.find('abstract')\n",
    "    for p in abt.findAll('p'):\n",
    "        text = text + p.text + \" \"\n",
    "    \n",
    "    #Extract Claims \n",
    "    for tag in xmlSoup.find_all('claim-text'):\n",
    "        text = text + tag.text\n",
    " \n",
    "    return (pid, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patents_raw = {}\n",
    "for xfile in os.listdir(xml_file_path): \n",
    "    xfile = os.path.join(xml_file_path, xfile)\n",
    "    if os.path.isfile(xfile) and xfile.endswith('.XML'): \n",
    "        (pid, text) = parsing(open(xfile))\n",
    "        patents_raw[pid] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenize the patents\n",
    "After finish extract the texts, you now need to tokenize the patents with regular expression tokenizer implemented in NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizePatent(pid):\n",
    "    \"\"\"\n",
    "        the tokenization function is used to tokenize each patent.\n",
    "        The one argument is patent_id.\n",
    "        First, normalize the case.\n",
    "        Then, use the regular expression tokenizer to tokenize the patent with the specified id\n",
    "    \"\"\"\n",
    "    raw_patent = patents_raw[pid].lower() \n",
    "    tokenized_patents = tokenizer.tokenize(raw_patent)\n",
    "    return (pid, tokenized_patents) # return a tupel of patent_id and a list of tokens\n",
    "\n",
    "patents_tokenized = dict(tokenizePatent(pid) for pid in patents_raw.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Generate the 100 bigram collocations\n",
    "The next task is go generate the bigram collocations, given the tokenized patents.\n",
    "\n",
    "The first step is to concatenate all the tokenized patents using the chain.frome_iterable function. The returned list \n",
    "by the function contains a list of all the words seprated by while space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens:  140764\n",
      "the size of the unigram vocabulary:  3318 \n",
      "\n",
      "['feed', 'inhibit', 'converter', 'grass', 'precluding', 'additives', 'otherwise', 'increasing', 'design', 'disengages', 'temporary', 'slide', 'wetter', 'catalytic', 'transport', 'vent', 'relates', 'forwards', 'produced', 'oxygen', 'without', 'starter', 'sleeves', 'issue', 'waterproofed', 'controls', 'axially', 'limiting', 'brought', 'protrusions', 'forth', 'handling', 'arrayed', 'securing', 'high', 'recording', 'walled', 'connective', 'returning', 'direction', 'grooves', 'strand', 'effective', 'apart', 'snag', 'envelop', 'but', 'base', 'tilting', 'ceramic']\n"
     ]
    }
   ],
   "source": [
    "all_words = list(chain.from_iterable(patents_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to generate the 100 bigram cllocations. The functions you need include\n",
    "* BigramAssocMeasures()\n",
    "* BigramCollocationFinder.from_words()\n",
    "* apply_freq_filter(20)\n",
    "* apply_word_filter(lambda w: len(w) < 3)\n",
    "* nbest(bigram_measures.pmi, 100)\n",
    "\n",
    "Please do not change the parameters given in the last three function. More information about generating collocation with NLTK can be found http://www.nltk.org/howto/collocations.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('harmonic', 'flex'),\n",
       " ('centrifugally', 'balanced'),\n",
       " ('robotic', 'harmonic'),\n",
       " ('expandable', 'chuck'),\n",
       " ('group', 'consisting'),\n",
       " ('charge', 'consistent'),\n",
       " ('walk', 'behind'),\n",
       " ('improperly', 'swapped'),\n",
       " ('saw', 'resonator'),\n",
       " ('behind', 'mowing'),\n",
       " ('jute', 'fibers'),\n",
       " ('actuator', 'flag'),\n",
       " ('elastic', 'band'),\n",
       " ('lead', 'frames'),\n",
       " ('drain', 'vent'),\n",
       " ('fresh', 'food'),\n",
       " ('high', 'humidity'),\n",
       " ('paper', 'particles'),\n",
       " ('fringe', 'maker'),\n",
       " ('ultrasonic', 'test'),\n",
       " ('foot', 'pedal'),\n",
       " ('elastomeric', 'mat'),\n",
       " ('capacitor', 'devices'),\n",
       " ('loaded', 'bag'),\n",
       " ('hammermilled', 'straw'),\n",
       " ('flash', 'tank'),\n",
       " ('tank', 'receiver'),\n",
       " ('hip', 'joint'),\n",
       " ('does', 'not'),\n",
       " ('duty', 'belt'),\n",
       " ('drier', 'solid'),\n",
       " ('solid', 'phase'),\n",
       " ('removable', 'joining'),\n",
       " ('cooler', 'box'),\n",
       " ('not', 'exceed'),\n",
       " ('cross', 'sectional'),\n",
       " ('case', 'packer'),\n",
       " ('vacuum', 'electronic'),\n",
       " ('driver', 'pulley'),\n",
       " ('mowing', 'machine'),\n",
       " ('fastened', 'together'),\n",
       " ('storage', 'capacity'),\n",
       " ('bus', 'bars'),\n",
       " ('provide', 'circulating'),\n",
       " ('set', 'forth'),\n",
       " ('force', 'translation'),\n",
       " ('lock', 'guard'),\n",
       " ('relatively', 'drier'),\n",
       " ('outwardly', 'protruding'),\n",
       " ('hand', 'crank'),\n",
       " ('well', 'plug'),\n",
       " ('coil', 'wires'),\n",
       " ('threaded', 'stud'),\n",
       " ('condemned', 'condition'),\n",
       " ('living', 'hinge'),\n",
       " ('ending', 'point'),\n",
       " ('magazine', 'well'),\n",
       " ('spaced', 'apart'),\n",
       " ('test', 'probe'),\n",
       " ('incline', 'angle'),\n",
       " ('retainer', 'tab'),\n",
       " ('liquid', 'dielectric'),\n",
       " ('shift', 'drum'),\n",
       " ('wringing', 'tube'),\n",
       " ('flex', 'drive'),\n",
       " ('timing', 'pulley'),\n",
       " ('starting', 'point'),\n",
       " ('relatively', 'wet'),\n",
       " ('shift', 'spindle'),\n",
       " ('bus', 'bar'),\n",
       " ('moisture', 'removal'),\n",
       " ('photoreceptor', 'substrate'),\n",
       " ('cat', 'oxygen'),\n",
       " ('fuel', 'ratio'),\n",
       " ('elongate', 'strength'),\n",
       " ('securing', 'connector'),\n",
       " ('notch', 'cut'),\n",
       " ('time', 'period'),\n",
       " ('directly', 'opposite'),\n",
       " ('both', 'sides'),\n",
       " ('less', 'than'),\n",
       " ('multi', 'sensing'),\n",
       " ('selectively', 'fastened'),\n",
       " ('post', 'cat'),\n",
       " ('catalytic', 'converter'),\n",
       " ('joining', 'means'),\n",
       " ('sectional', 'shape'),\n",
       " ('winding', 'arms'),\n",
       " ('mainspring', 'pulley'),\n",
       " ('clamping', 'face'),\n",
       " ('active', 'state'),\n",
       " ('about', 'percent'),\n",
       " ('sixth', 'strap'),\n",
       " ('shutter', 'panel'),\n",
       " ('composite', 'rods'),\n",
       " ('upright', 'post'),\n",
       " ('food', 'compartment'),\n",
       " ('pivotally', 'coupled'),\n",
       " ('determining', 'whether'),\n",
       " ('arm', 'rests')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "bigram_finder.apply_freq_filter(20)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "top_100_bigrams = bigram_finder.nbest(bigram_measures.pmi, 100) # Top-100 bigrams\n",
    "top_100_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Re-tokenize the patents again.\n",
    "\n",
    "Task in Section 4 takenise the patents with only unigrams. Now, we introduce 100 collcations. we need to make sure those collocations are not split into two individual words. The tokenizer that you need is <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">MWEtokenizer</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372\n"
     ]
    }
   ],
   "source": [
    "mwetokenizer = MWETokenizer(top_100_bigrams)\n",
    "colloc_patents =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in patents_tokenized.items())\n",
    "all_words_colloc = list(chain.from_iterable(colloc_patents.values()))\n",
    "colloc_voc = list(set(all_words_colloc))\n",
    "print(len(colloc_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the difference between th output of MWEtokenizer and RegexpTokenizer by <font size=3>adpating</font> the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "for pid in patents_tokenized.keys():\n",
    "    diff = set(colloc_patents[pid])-set(patents_tokenized[pid])\n",
    "    if len(diff) != 0:\n",
    "        print (pid, diff)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate the TF-IDF vectors for all the patents.\n",
    "Please refer to \n",
    "* http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pids = []\n",
    "patent_words = []\n",
    "for pid, tokens in colloc_patents.items():\n",
    "    pids.append(pid)\n",
    "    txt = ' '.join(tokens)\n",
    "    patent_words.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3372)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(patent_words)\n",
    "tfidf_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the TF-IDF vector into the specified format\n",
    "Hint: you can use \n",
    "* the <a href=\"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csc_matrix.tocoo.html\">tocoo()</a> function\n",
    "* itertools.zip_longest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file = open(\"patent_student.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "#########please write the missing code below#######\n",
    "cx = tfidf_vectors.tocoo() # return the coordinate representation of a sparse matrix\n",
    "for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
    "    save_file.write(pids[i] + ',' + vocab[j] + ',' + str(v) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
