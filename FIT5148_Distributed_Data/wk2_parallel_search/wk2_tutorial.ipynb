{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## FIT5148 - Distributed Databases and Big Data\n",
    "### Activity: Parallel Search\n",
    "\n",
    "In this activity, we will learn and build different parallel search algorithms on various data partitioning strategies. This work will help you to better understand and familiarise you with how parallel search algorithms can work and be implemented.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "   - You will be using Python 3.\n",
    "   - Read the code base and comments carefully\n",
    "   - After understanding the provided function, run the cell right below it to check if the result is correct.\n",
    "   - Read carefully all the Exercise tasks below in each subheading. There are some code blocks that you need to complete yourself.\n",
    "\n",
    "After this assignment you will:\n",
    "\n",
    "   - Be able to use iPython Notebooks\n",
    "   - Be able to build data partitionining strategies\n",
    "   - Be able to build basic search algorithms\n",
    "   - Be able to understand and build parallel search algorithms based on data partitioning techniques and basic search algorithms\n",
    "\n",
    "Let's get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 30, 68, 39, 1, 4, 49, 90, 34, 76, 82, 56, 25, 23, 78, 56, 38, 32, 88, 9, 44, 98, 11, 70, 66, 89, 99, 22, 31, 26]\n"
     ]
    }
   ],
   "source": [
    "# Our example dataset D consisting of 30 numeric elements.\n",
    "D = [55,30,68,39,1,\n",
    "      4,49,90,34,76,\n",
    "      82,56,25,23,78,\n",
    "      56,38,32,88,9,\n",
    "      44,98,11,70,66,\n",
    "      89,99,22,31,26]\n",
    "\n",
    "print(D) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Data Partitioning\n",
    "\n",
    "Data partitioning is the fundamental step for parallel search algorithms as parallelism in query and search processing is achieved through data partionining. In this activity, we will consider the following four partitioning strategies:\n",
    "\n",
    "   - Round-robin data partitioning,\n",
    "   - Hash data partitioning,\n",
    "   - Range data partitioning, and\n",
    "   - Random-unequal data partitioning\n",
    "\n",
    "1.1 Round-robin data partitioning\n",
    "\n",
    "Round-robin data partitioning is the simplest data partitioning method in which each record in turn is allocated to a processing element (simply processor). Since it distributes the data evenly among all processors, it is also known as \"equal-partitioning\".\n",
    "\n",
    "**Exercise**: Understand the following code of round-robin data partitioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-robin data partitionining function\n",
    "def rr_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    #n_bin = len(data)/n\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = (int) (index % n)\n",
    "        #print(str(index) + \":\" + str(element))\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55, 39, 49, 76, 25, 56, 88, 98, 66, 22],\n",
       " [30, 1, 90, 82, 23, 38, 9, 11, 89, 31],\n",
       " [68, 4, 34, 56, 78, 32, 44, 70, 99, 26]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "rr_partition(D, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "\n",
    "    Arguments:\n",
    "    x -- an input record\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    result = x%n \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a hash value\n",
    "s_hash(11, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash data partitionining function. \n",
    "# We will use the \"s_hash\" function defined above to realise this partitioning\n",
    "def h_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform hash data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    dic = {} # We will use a dictionary\n",
    "    for x in data: # For each data record, perform the following\n",
    "        h = s_hash(x, n) # Get the hash key of the input\n",
    "        if h in dic.keys(): # If the key exists\n",
    "            # Add the new input to the value set of the key\n",
    "            dic[h].add(x)       \n",
    "        else: # If the key does not exist\n",
    "            s = set() # Create an empty value set\n",
    "            s.update({x})\n",
    "            dic[h] = s # Add the value set to the key\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {9, 30, 39, 66, 78, 90, 99},\n",
       " 1: {1, 4, 22, 25, 31, 34, 49, 55, 70, 76, 82, 88},\n",
       " 2: {11, 23, 26, 32, 38, 44, 56, 68, 89, 98}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "h_partition(D, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Range data partitioning\n",
    "\n",
    "Range data partitioning records based on a given range of the partitioning attribute. For example,the student table is partitioned based on \"Last Name\" based on the alphabetical order (i.e. A ~ Z).\n",
    "\n",
    "**Exercise**: Understand the following code of range data partitioning. As our dataset D is represented by numerical values, we partition D according to numeric values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Range data partitionining function\n",
    "def range_partition(data, range_indices):\n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    range_indices -- the index list of ranges to be split\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    # First, we sort the dataset according their values  \n",
    "    new_data = list(data)\n",
    "    new_data.sort()\n",
    "\n",
    "    # Calculate the number of bins\n",
    "    n_bin = len(range_indices) \n",
    "\n",
    "    # For each bin, perform the following\n",
    "    for i in range(n_bin): \n",
    "        # Find elements to be belonging to each range\n",
    "        s = [x for x in new_data if x < range_indices[i]] \n",
    "        # Add the partitioned list to the result\n",
    "        result.append(s) \n",
    "        # Find the last element in the previous partition\n",
    "        last_element = s[len(s)-1]\n",
    "        # Find the index of of the last element\n",
    "        last = new_data.index(last_element)\n",
    "        # Remove the partitioned list from the dataset\n",
    "        new_data = new_data[int(last)+1:] \n",
    "\n",
    "        # Append the last remaining data list\n",
    "    result.append([x for x in new_data if x >= range_indices[n_bin-1]]) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 9, 11, 22, 23, 25, 26, 30, 31, 32, 34, 38, 39],\n",
       " [44, 49, 55, 56, 56, 66, 68, 70, 76, 78],\n",
       " [82, 88, 89, 90, 98, 99]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result\n",
    "range_partition(D, [40, 80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 Random-unequal data partitioning\n",
    "\n",
    "In random-unequal data partitioning, the size of each partition is likely to be unequal. The word “random” in the name indicates that the records within each partition are not grouped semantically, but are randomly allocated.\n",
    "\n",
    "**Exercise**: Implement random-unequal data partitioning based on your definition. Referring to the function, rr_partition(), complete the following code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random-unequal data partitionining function\n",
    "import numpy as np\n",
    "def re_partition(data, n):\n",
    "    \"\"\"\n",
    "    Perform random-unequal data partitioning on data\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    n -- the number of processors\n",
    "\n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append([])\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    # Calculate the number of the elements to be allocated to each bin\n",
    "    n_bin = len(data)/n\n",
    "\n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for index, element in enumerate(data): \n",
    "        # Calculate the index of the bin that the current data point will be assigned\n",
    "        index_bin = np.random.randint(0,n)\n",
    "        #print(str(index) + \":\" + str(element))\n",
    "        result[index_bin].append(element)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 1, 49, 25, 78, 38, 32, 44, 66, 89, 99],\n",
       " [55, 68, 4, 34, 82, 56, 88, 9, 70, 22],\n",
       " [30, 90, 76, 23, 56, 98, 11, 31, 26]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the partitioned result. \n",
    "# Compare the result with the one obtained from rr_partition(.).\n",
    "re_partition(D, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2: Search Algorithms\n",
    "\n",
    "Before discussing parallel search, it is important to know how searching is done serially. Making use of serial search algorithms with data partitioning will become the basis for parallel search algorithms. In this activity, we will consider the following two serial search algorithms:\n",
    "\n",
    "   - Linear Search\n",
    "   - Binary Search\n",
    "\n",
    "### 2.1 Linear Search\n",
    "\n",
    "Linear search is the simplest approach to searching. Given an unsorted table of records, it scans the entire table to search for a given record. As this is performed for each record one by one until either the desired record is found or the end of table is reached, this algorithms is also known as an “exhaustive search.”\n",
    "\n",
    "**Exercise**: We use the dataset D to understand how this algorithm works. Each element in D will be considered as a data record. Let's understand how linear search works on D, and analyse its performance by the \"O\" notation which is normally used to measure the complexity of an algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear search function\n",
    "def linear_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform linear search on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list or a numpy array\n",
    "    key -- an query record\n",
    "\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_record = None\n",
    "    position = -1 # not found position\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for x in data:\n",
    "        if x == key: # If x is matched with key\n",
    "            matched_record = x\n",
    "            position = data.index(x) # Get the index of x\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return position, matched_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_search (D, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The Big O notation of linear search is O(n), which can be obviuosly infered from its name linear, the time complexity is linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Binary Search\n",
    "\n",
    "Binary search requires that the list be already completely in order. It starts by comparing the key with the middle entry of an ordered table. If it finds the matched record, it returns its index, otherwise, this process continues using either the lower or upper half of the table (depending on the value of the key).\n",
    "\n",
    "**Exercise**: Build a binary search function by completing the code block below between \"### START CODE HERE ###\" and \"### END CODE HERE ###\". Discuss its complexity by comparing with the linear search algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary search function\n",
    "def binary_search(data, key):\n",
    "    \"\"\"\n",
    "    Perform binary search on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    key -- an query record\n",
    "\n",
    "    Return:\n",
    "    result -- the position of searched record\n",
    "    \"\"\"\n",
    "    \n",
    "    matched_record = None\n",
    "    position = -1 # not found position\n",
    "    \n",
    "    lower = 0\n",
    "    #middle = 0\n",
    "    upper = len(data)-1\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    while lower <= upper and not matched_record:\n",
    "        # calculate middle: the half of lower and upper\n",
    "        middle = int((lower + upper)/2) \n",
    "        if data[middle] == key:\n",
    "            position = middle\n",
    "            matched_record = data[middle]\n",
    "        elif data[middle] < key:\n",
    "            lower = middle + 1\n",
    "        else:\n",
    "            upper = middle - 1\n",
    "                \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return position, matched_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 31)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortD = list(D) # Copy the dataset\n",
    "sortD.sort() # Sort the dataset first\n",
    "binary_search (sortD, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_search(D,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3: Parallel Search Algorithms\n",
    "\n",
    "Parallel search algorithms have three main elements:\n",
    "\n",
    "   - processor activation or involvement\n",
    "   - local searching method\n",
    "   - key comparison\n",
    "\n",
    "Processor activation or involvement indicates the number of processors to be used by the algorithm.\n",
    "\n",
    "Local searching method is the searching method to be applied to the processor(s). The search method is dependent upon the data ordering. If the data has already been sorted, then a binary search can be used, and otherwise, a linear search can be conducted.\n",
    "\n",
    "Searching basically consists of comparing the data from the table with the condition specified by the user. When a match is found, there are two options: whether to continue the comparison process in order to find more matches, or whether to stop the entire process. It is obvious that the key comparison is dependent upon whether the search attribute values are, or are not, unique.\n",
    "3.1 Parallel Searching for Exact Match\n",
    "\n",
    "In this activity, we will understand and practice how parallel searching works for exact match search for a given query. Note that the number of processors to perform parallel searching is dependent on the data partitioning methods. For example, only one processor is needed if the data is already partioned with a range partitioning. In this case, there is no parallelism.\n",
    "\n",
    "**Exercise**: We build a parallel search algorithm for exact match. Processor activation will be given by the user as input. As a location searching method, we will use the above two search functions: linear search function (i.e. linear_search()) and binary search function (i.e. binary_search()). As a local comparison method, we assume that we stop when a match is found for brevity. As data partitioning methods, we attempt to use the four different partitioning methods we built above:\n",
    "\n",
    "   - Round-robin data partitioning (i.e. rr_partition()),\n",
    "   - Hash data partitioning (i.e. h_partition()),\n",
    "   - Range data partitioning (i.e. range_partition()), and\n",
    "   - Random-unequal data partitioning (i.e. re_partition())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool # For multiprocessing\n",
    "\n",
    "# Parallel searching algorithm for exact match\n",
    "def parallel_search_exact(data, query, n_processor, m_partition, m_search):\n",
    "    \"\"\"\n",
    "    Perform parallel search for exact match on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    query -- a query record\n",
    "    n_processor -- the number of parallel processors\n",
    "    m_partition -- a data partitioning method\n",
    "    m_search -- a search method\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pool: a Python method enabling parallel processing. \n",
    "    # We need to set the number of processes to n_processor, \n",
    "    # which means that the Pool class will only allow 'n_processor' processes \n",
    "    # running at the same time.\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###        \n",
    "    \n",
    "    print(\"data partitioning:\" + str(m_partition.__name__))\n",
    "    print(\"searching method:\" + str(m_search.__name__))\n",
    "\n",
    "    if m_partition == range_partition: # for range partitioning method\n",
    "        # Perform data partitioning:\n",
    "        # 2nd parameter is a list of maximum range values (3 ranges)\n",
    "        DD = m_partition(data, [40, 80]) \n",
    "        for d in DD: # Find the range that may contain the query \n",
    "            if query in d:\n",
    "                m = list(d)\n",
    "                result = pool.apply(m_search, (m, query))\n",
    "                results.append(result)    \n",
    "                break\n",
    "    \n",
    "    elif m_partition == h_partition: # for hash partitioning method\n",
    "        # Perform data partitioning first\n",
    "        DD = m_partition(data, n_processor) \n",
    "        # Each element in DD has a pair (hash key: records)\n",
    "        query_hash = s_hash(query, n_processor)\n",
    "        d = list(DD[query_hash])\n",
    "        result = pool.apply(m_search, [d, query])\n",
    "        results.append(result)\n",
    "\n",
    "    else: # for round-robin or random-unequal partitioning method\n",
    "        # Perform data partitioning first\n",
    "        DD = m_partition(data, n_processor)         \n",
    "        for d in DD: # Perform parallel search on all data partitions\n",
    "            result = pool.apply(m_search, [d, query])\n",
    "            #output = result.get() # if you use pool.apply_async(), uncomment this.\n",
    "            #results.append(output) # if you use pool.apply_async(), uncomment this.\n",
    "            results.append(result) # if you use pool.apply_async(), comment out this.\n",
    "        \n",
    "    \"\"\" \n",
    "    The method above 'pool.apply()' will lock the function program until all a process \n",
    "    is finished. Alternatively, we can use the 'pool.apply_async()' method \n",
    "    to spawn one process for each CPU core on your machine.\n",
    "    \"\"\"\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data partitioning:rr_partition\n",
      "searching method:linear_search\n",
      "[(-1, None), (9, 31), (-1, None)]\n",
      "data partitioning:rr_partition\n",
      "searching method:binary_search\n",
      "[(-1, None), (-1, None), (-1, None)]\n",
      "data partitioning:re_partition\n",
      "searching method:linear_search\n",
      "[(13, 31), (-1, None), (-1, None)]\n",
      "data partitioning:re_partition\n",
      "searching method:binary_search\n",
      "[(-1, None), (-1, None), (-1, None)]\n"
     ]
    }
   ],
   "source": [
    "# Common input values\n",
    "data = D # input data\n",
    "sortD = list(data)\n",
    "sortD.sort()\n",
    "query = 31 # query term\n",
    "n_processor = 3 # number of parallel processors\n",
    "\n",
    "### parallel searching for exact match\n",
    "\n",
    "# round-robin partition, linear_search\n",
    "print(parallel_search_exact(data,query,n_processor,rr_partition,linear_search))\n",
    "# round-robin partition, binary_search\n",
    "print(parallel_search_exact(data,query,n_processor,rr_partition,binary_search))\n",
    "# random-unequal partition, linear_search\n",
    "print(parallel_search_exact(data,query,n_processor,re_partition,linear_search))\n",
    "# random-unequal partition, binary_search \n",
    "print(parallel_search_exact(data,query,n_processor,re_partition,binary_search))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data partitioning:h_partition\n",
      "searching method:binary_search\n",
      "[(-1, None)]\n",
      "data partitioning:h_partition\n",
      "searching method:binary_search\n",
      "[(-1, None)]\n",
      "data partitioning:range_partition\n",
      "searching method:binary_search\n",
      "[(9, 31)]\n",
      "data partitioning:range_partition\n",
      "searching method:binary_search\n",
      "[(9, 31)]\n"
     ]
    }
   ],
   "source": [
    "# Hash partition, linear_search \n",
    "print(parallel_search_exact(data,query,n_processor,h_partition,binary_search))\n",
    "# Hash partition, binary_search \n",
    "print(parallel_search_exact(data,query,n_processor,h_partition,binary_search))\n",
    "# Range partition, linear_search \n",
    "print(parallel_search_exact(data,query,n_processor,range_partition,binary_search))\n",
    "# Range partition, binary_search \n",
    "print(parallel_search_exact(data,query,n_processor,range_partition,binary_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: What do we see from the results? We see the set of the pairs each consistings of the position and value of the matched recored given a query. The -1 position indicates the query was not found. If found, a position is > -1.\n",
    "### 3.2 Parallel Searching for Range Selection (Continuous)\n",
    "In this activity, we will build a parallel search algorithm for range selection (continuous) for a given query. In this practice, we attempt to implement one particular search algorithm which is instructed below.\n",
    "\n",
    "**Exercise**: Build a parallel search algorithm that uses the linear search algorithm (i.e. linear_search()) and is able to work with the hash partitioning method (i.e. h_partition()). Complete the code block between \"### START CODE HERE ###\" and \"### END CODE HERE ###\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parallel searching algorithm for range selection\n",
    "def parallel_search_range(data, query_range, n_processor):\n",
    "    \"\"\"\n",
    "    Perform parallel search for range selection on data for the given key\n",
    "\n",
    "    Arguments:\n",
    "    data -- the input dataset which is a list\n",
    "    query_range -- a query record in the form of a range (e.g. [30, 50])\n",
    "    n_processor -- the number of parallel processors\n",
    "    \n",
    "    Return:\n",
    "    results -- the matched record information\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    pool = Pool(processes=n_processor)\n",
    "\n",
    "    ### START CODE HERE ###        \n",
    "    D = h_partition(data,n_processor)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 in range(2,8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
